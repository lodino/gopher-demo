{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T20:35:32.634414Z",
     "start_time": "2021-10-18T20:35:31.526746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16ec04830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "import time\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from load_dataset import load\n",
    "from classifier import NeuralNetwork, LogisticRegression, SVM\n",
    "from utils import *\n",
    "from metrics import *  # include fairness and corresponding derivatives\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from operator import itemgetter\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T20:35:32.637761Z",
     "start_time": "2021-10-18T20:35:32.635822Z"
    }
   },
   "outputs": [],
   "source": [
    "# ignore all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:16.817746Z",
     "start_time": "2021-10-18T22:26:16.777450Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "dataset = 'german'\n",
    "X_train, X_test, y_train, y_test = load(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:17.787141Z",
     "start_time": "2021-10-18T22:26:17.767989Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# size=500\n",
    "# X_train = X_train[0:size]\n",
    "# y_train = y_train[0:size]\n",
    "\n",
    "X_train_orig = copy.deepcopy(X_train)\n",
    "X_test_orig = copy.deepcopy(X_test)\n",
    "\n",
    "# Scale data: regularization penalty default: ‘l2’, ‘lbfgs’ solvers support only l2 penalties. \n",
    "# Regularization makes the predictor dependent on the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** (Log loss for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:17.943658Z",
     "start_time": "2021-10-18T22:26:17.935204Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf = NeuralNetwork(input_size=X_train.shape[-1])\n",
    "clf = LogisticRegression(input_size=X_train.shape[-1])\n",
    "# clf = SVM(input_size=X_train.shape[-1])\n",
    "num_params = len(convert_grad_to_ndarray(list(clf.parameters())))\n",
    "if isinstance(clf, LogisticRegression) or isinstance(clf, NeuralNetwork):\n",
    "#     loss_func = lambda model, x, y_true: logistic_loss_torch(model(torch.FloatTensor(x)),\\\n",
    "#                                                              torch.FloatTensor([y_true])) +\\\n",
    "#     model.C*torch.sqrt(torch.sum(convert_grad_to_tensor(list(clf.parameters()))**2))\n",
    "    loss_func = logistic_loss_torch\n",
    "elif isinstance(clf, SVM):\n",
    "    loss_func = svm_loss_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Influence of points computed using ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:18.115411Z",
     "start_time": "2021-10-18T22:26:18.103329Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ground_truth_influence(X_train, y_train, X_test, X_test_orig, y_test):\n",
    "    clf.fit(X_train, y_train, verbose=True)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    spd_0 = computeFairness(y_pred, X_test_orig, y_test, 0)\n",
    "\n",
    "    delta_spd = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_removed = np.delete(X_train, i, 0)\n",
    "        y_removed = y_train.drop(index=i, inplace=False)\n",
    "        clf.fit(X_removed, y_removed)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        delta_spd_i = computeFairness(y_pred, X_test_orig, y_test, 0) - spd_0\n",
    "        delta_spd.append(delta_spd_i)\n",
    "\n",
    "    return delta_spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Accuracy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:18.284002Z",
     "start_time": "2021-10-18T22:26:18.277564Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def computeAccuracy(y_true, y_pred):\n",
    "    return np.sum((y_pred>0.5) == y_true)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of loss function at z with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:18.449601Z",
     "start_time": "2021-10-18T22:26:18.441596Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_L_del_theta_i(model, x, y_true, retain_graph=False):\n",
    "    loss = loss_func(model, x, y_true)\n",
    "    w = [ p for p in model.parameters() if p.requires_grad ]\n",
    "    return grad(loss, w, create_graph=True, retain_graph=retain_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order derivative of $P(y \\mid \\textbf{x})$ with respect to model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:18.615860Z",
     "start_time": "2021-10-18T22:26:18.608771Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_f_del_theta_i(model, x, retain_graph=False):\n",
    "    w = [ p for p in model.parameters() if p.requires_grad ]\n",
    "    return grad(model(torch.FloatTensor(x)), w, retain_graph=retain_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic estimation of Hessian vector product (involving del fairness): $H_{\\theta}^{-1}v = H_{\\theta}^{-1}\\nabla_{\\theta}f(z, \\theta) = v + [I - \\nabla_{\\theta}^2L(z_{s_j}, \\theta^*)]H_{\\theta}^{-1}v$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:18.780627Z",
     "start_time": "2021-10-18T22:26:18.774308Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def hvp(y, w, v):\n",
    "    ''' Multiply the Hessians of y and w by v.'''\n",
    "    # First backprop\n",
    "    first_grads = grad(y, w, retain_graph=True, create_graph=True)\n",
    "\n",
    "    # Elementwise products\n",
    "    elemwise_products = 0\n",
    "    for grad_elem, v_elem in zip(convert_grad_to_tensor(first_grads), v):\n",
    "        elemwise_products += torch.sum(grad_elem * v_elem)\n",
    "\n",
    "    # Second backprop\n",
    "    return_grads = grad(elemwise_products, w, create_graph=True)\n",
    "\n",
    "    return return_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:18.869155Z",
     "start_time": "2021-10-18T22:26:18.858483Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def hessian_one_point(model, x, y):\n",
    "    x, y = torch.FloatTensor(x), torch.FloatTensor([y])\n",
    "    loss = loss_func(model, x, y)\n",
    "    params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "    first_grads = convert_grad_to_tensor(grad(loss, params, retain_graph=True, create_graph=True))\n",
    "    hv = np.zeros((len(first_grads), len(first_grads)))\n",
    "    for i in range(len(first_grads)):\n",
    "        hv[i, :] = convert_grad_to_ndarray(grad(first_grads[i], params, create_graph=True)).ravel()\n",
    "    return hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:18.961112Z",
     "start_time": "2021-10-18T22:26:18.942137Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Compute multiplication of inverse hessian matrix and vector v\n",
    "def s_test(model, xs, ys, v, hinv=None, damp=0.01, scale=25.0, r=-1, batch_size=-1, recursive=False, verbose=False):\n",
    "    ''' Arguments:\n",
    "        xs: list of data points\n",
    "        ys: list of true labels corresponding to data points in xs\n",
    "        damp: dampening factor\n",
    "        scale: scaling factor\n",
    "        r: number of iterations aka recursion depth\n",
    "            should be enough so that the value stabilises.\n",
    "        batch_size: number of instances in each batch in recursive approximation\n",
    "        recursive: determine whether to recursively approximate hinv_v'''\n",
    "    xs, ys = torch.FloatTensor(xs.copy()), torch.FloatTensor(ys.copy())\n",
    "    n = len(xs)\n",
    "    if recursive:\n",
    "        hinv_v = copy.deepcopy(v)\n",
    "        if verbose:\n",
    "            print('Computing s_test...')\n",
    "            tbar = tqdm.tqdm(total=r)\n",
    "        if (batch_size == -1):  # default\n",
    "            batch_size = 10\n",
    "        if (r == -1):\n",
    "            r = n // batch_size + 1\n",
    "        sample = np.random.choice(range(n), r*batch_size, replace=True)\n",
    "        for i in range(r):\n",
    "            sample_idx = sample[i*batch_size:(i+1)*batch_size]\n",
    "            x, y = xs[sample_idx], ys[sample_idx]\n",
    "            loss = loss_func(model, x, y)\n",
    "            params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "            hv = convert_grad_to_ndarray(hvp(loss, params, torch.FloatTensor(hinv_v)))\n",
    "            # Recursively caclulate h_estimate\n",
    "            hinv_v = v + (1 - damp) * hinv_v - hv / scale\n",
    "            if verbose:\n",
    "                tbar.update(1)\n",
    "    else:\n",
    "        if hinv is None:\n",
    "            hinv = np.linalg.pinv(np.sum(hessian_all_points, axis=0))\n",
    "        scale = 1.0\n",
    "        hinv_v = np.matmul(hinv, v)\n",
    "\n",
    "    return hinv_v / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics: Initial state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:19.142447Z",
     "start_time": "2021-10-18T22:26:19.107404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial statistical parity:  -0.09443411026149984\n",
      "Initial TPR parity:  -0.07578001513974653\n",
      "Initial predictive parity:  -0.10184434840602907\n",
      "Initial loss:  0.5011112929531027\n",
      "Initial accuracy:  0.755\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(input_size=X_train.shape[-1])\n",
    "# clf = NeuralNetwork(input_size=X_train.shape[-1])\n",
    "# clf = SVM(input_size=X_train.shape[-1])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_train = clf.predict_proba(X_train)\n",
    "\n",
    "spd_0 = computeFairness(y_pred_test, X_test_orig, y_test, 0, dataset)\n",
    "print(\"Initial statistical parity: \", spd_0)\n",
    "\n",
    "tpr_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 1, dataset)\n",
    "print(\"Initial TPR parity: \", tpr_parity_0)\n",
    "\n",
    "predictive_parity_0 = computeFairness(y_pred_test, X_test_orig, y_test, 2, dataset)\n",
    "print(\"Initial predictive parity: \", predictive_parity_0)\n",
    "\n",
    "loss_0 = logistic_loss(y_test, y_pred_test)\n",
    "print(\"Initial loss: \", loss_0)\n",
    "\n",
    "accuracy_0 = computeAccuracy(y_test, y_pred_test)\n",
    "print(\"Initial accuracy: \", accuracy_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.431794Z",
     "start_time": "2021-10-18T22:26:19.193140Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48605/48605 [1:50:45<00:00,  7.31it/s]\u001b[A\n",
      "\n",
      "  2%|███                                                                                                                                                                  | 15/800 [00:00<00:05, 142.90it/s]\u001b[A\n",
      "  4%|██████▏                                                                                                                                                              | 30/800 [00:00<00:05, 144.38it/s]\u001b[A\n",
      "  6%|█████████▎                                                                                                                                                           | 45/800 [00:00<00:05, 145.61it/s]\u001b[A\n",
      "  8%|████████████▍                                                                                                                                                        | 60/800 [00:00<00:05, 145.87it/s]\u001b[A\n",
      " 10%|███████████████▋                                                                                                                                                     | 76/800 [00:00<00:04, 148.76it/s]\u001b[A\n",
      " 12%|██████████████████▉                                                                                                                                                  | 92/800 [00:00<00:04, 149.94it/s]\u001b[A\n",
      " 14%|██████████████████████▏                                                                                                                                             | 108/800 [00:00<00:04, 151.22it/s]\u001b[A\n",
      " 16%|█████████████████████████▍                                                                                                                                          | 124/800 [00:00<00:04, 152.51it/s]\u001b[A\n",
      " 18%|████████████████████████████▋                                                                                                                                       | 140/800 [00:00<00:04, 152.42it/s]\u001b[A\n",
      " 20%|███████████████████████████████▉                                                                                                                                    | 156/800 [00:01<00:04, 152.57it/s]\u001b[A\n",
      " 22%|███████████████████████████████████▎                                                                                                                                | 172/800 [00:01<00:04, 153.68it/s]\u001b[A\n",
      " 24%|██████████████████████████████████████▌                                                                                                                             | 188/800 [00:01<00:04, 152.19it/s]\u001b[A\n",
      " 26%|█████████████████████████████████████████▊                                                                                                                          | 204/800 [00:01<00:03, 152.02it/s]\u001b[A\n",
      " 28%|█████████████████████████████████████████████                                                                                                                       | 220/800 [00:01<00:03, 153.47it/s]\u001b[A\n",
      " 30%|████████████████████████████████████████████████▍                                                                                                                   | 236/800 [00:01<00:03, 155.14it/s]\u001b[A\n",
      " 32%|███████████████████████████████████████████████████▋                                                                                                                | 252/800 [00:01<00:03, 154.98it/s]\u001b[A\n",
      " 34%|██████████████████████████████████████████████████████▉                                                                                                             | 268/800 [00:01<00:03, 154.17it/s]\u001b[A\n",
      " 36%|██████████████████████████████████████████████████████████▏                                                                                                         | 284/800 [00:01<00:03, 155.15it/s]\u001b[A\n",
      " 38%|█████████████████████████████████████████████████████████████▌                                                                                                      | 300/800 [00:01<00:03, 154.67it/s]\u001b[A\n",
      " 40%|████████████████████████████████████████████████████████████████▊                                                                                                   | 316/800 [00:02<00:03, 152.10it/s]\u001b[A\n",
      " 42%|████████████████████████████████████████████████████████████████████                                                                                                | 332/800 [00:02<00:03, 152.68it/s]\u001b[A\n",
      " 44%|███████████████████████████████████████████████████████████████████████▎                                                                                            | 348/800 [00:02<00:02, 153.96it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████████████████████████████████████████▌                                                                                         | 364/800 [00:02<00:02, 155.02it/s]\u001b[A\n",
      " 48%|█████████████████████████████████████████████████████████████████████████████▉                                                                                      | 380/800 [00:02<00:02, 154.00it/s]\u001b[A\n",
      " 50%|█████████████████████████████████████████████████████████████████████████████████▏                                                                                  | 396/800 [00:02<00:02, 152.75it/s]\u001b[A\n",
      " 52%|████████████████████████████████████████████████████████████████████████████████████▍                                                                               | 412/800 [00:02<00:02, 152.91it/s]\u001b[A\n",
      " 54%|███████████████████████████████████████████████████████████████████████████████████████▋                                                                            | 428/800 [00:02<00:02, 152.98it/s]\u001b[A\n",
      " 56%|███████████████████████████████████████████████████████████████████████████████████████████                                                                         | 444/800 [00:02<00:02, 151.99it/s]\u001b[A\n",
      " 57%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                     | 460/800 [00:03<00:02, 150.90it/s]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                  | 476/800 [00:03<00:02, 151.58it/s]\u001b[A\n",
      " 62%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                               | 492/800 [00:03<00:02, 153.69it/s]\u001b[A\n",
      " 64%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 508/800 [00:03<00:01, 151.32it/s]\u001b[A\n",
      " 66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 524/800 [00:03<00:01, 152.73it/s]\u001b[A\n",
      " 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                     | 540/800 [00:03<00:01, 154.03it/s]\u001b[A\n",
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                  | 556/800 [00:03<00:01, 154.26it/s]\u001b[A\n",
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                              | 572/800 [00:03<00:01, 155.13it/s]\u001b[A\n",
      " 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                           | 588/800 [00:03<00:01, 154.76it/s]\u001b[A\n",
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 604/800 [00:03<00:01, 154.86it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                     | 620/800 [00:04<00:01, 154.85it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 636/800 [00:04<00:01, 154.78it/s]\u001b[A\n",
      " 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                              | 652/800 [00:04<00:00, 155.88it/s]\u001b[A\n",
      " 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                           | 668/800 [00:04<00:00, 156.60it/s]\u001b[A\n",
      " 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                       | 684/800 [00:04<00:00, 155.88it/s]\u001b[A\n",
      " 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 700/800 [00:04<00:00, 155.61it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 716/800 [00:04<00:00, 155.31it/s]\u001b[A\n",
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████              | 732/800 [00:04<00:00, 154.68it/s]\u001b[A\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 748/800 [00:04<00:00, 155.38it/s]\u001b[A\n",
      " 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊       | 765/800 [00:04<00:00, 157.10it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 781/800 [00:05<00:00, 157.65it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 797/800 [00:05<00:00, 157.77it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "hessian_all_points = []\n",
    "tbar = tqdm.tqdm(total=len(X_train))\n",
    "total_time = 0\n",
    "for i in range(len(X_train)):\n",
    "    t0 = time.time()\n",
    "    hessian_all_points.append(hessian_one_point(clf, X_train[i], y_train[i])/len(X_train))\n",
    "    total_time += time.time()-t0\n",
    "    tbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.436212Z",
     "start_time": "2021-10-18T22:26:24.433075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.149397373199463"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-compute: (1) Hessian (2) del_L_del_theta for each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.640062Z",
     "start_time": "2021-10-18T22:26:24.437596Z"
    }
   },
   "outputs": [],
   "source": [
    "del_L_del_theta = []\n",
    "for i in range(int(len(X_train))):\n",
    "    gradient = convert_grad_to_ndarray(del_L_del_theta_i(clf, X_train[i], int(y_train[i])))\n",
    "    while np.sum(np.isnan(gradient))>0:\n",
    "        gradient = convert_grad_to_ndarray(del_L_del_theta_i(clf, X_train[i], int(y_train[i])))\n",
    "    del_L_del_theta.append(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Select delta fairness function depending on selected metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.693983Z",
     "start_time": "2021-10-18T22:26:24.642071Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = 0\n",
    "if metric == 0:\n",
    "    v1 = del_spd_del_theta(clf, X_test_orig, X_test, dataset)\n",
    "elif metric == 1:\n",
    "    v1 = del_tpr_parity_del_theta(clf, X_test_orig, X_test, y_test, dataset)\n",
    "elif metric == 2:\n",
    "    v1 = del_predictive_parity_del_theta(clf, X_test_orig, X_test, y_test, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.703876Z",
     "start_time": "2021-10-18T22:26:24.694912Z"
    }
   },
   "outputs": [],
   "source": [
    "hinv = np.linalg.pinv(np.sum(hessian_all_points, axis=0))\n",
    "hinv_v = s_test(clf, X_train, y_train, v1, hinv=hinv, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.707959Z",
     "start_time": "2021-10-18T22:26:24.705071Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def first_order_influence(del_L_del_theta, hinv_v, n):\n",
    "    infs = []\n",
    "    for i in range(n):\n",
    "        inf = -np.dot(del_L_del_theta[i].transpose(), hinv_v)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf)\n",
    "    return infs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.711613Z",
     "start_time": "2021-10-18T22:26:24.708955Z"
    }
   },
   "outputs": [],
   "source": [
    "def first_order_group_influence(U, del_L_del_theta):\n",
    "    infs = []\n",
    "    u = len(U)\n",
    "    n = len(X_train)\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        inf = -np.dot(del_L_del_theta[idx].transpose(), hinv)\n",
    "        inf *= -1/n\n",
    "        infs.append(inf)\n",
    "    return np.sum(infs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second-order influence computation for a group of points in subset U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.717515Z",
     "start_time": "2021-10-18T22:26:24.712521Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def second_order_influence(model, X_train, y_train, U, del_L_del_theta, r=-1, verbose=False):\n",
    "    u = len(U)\n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(del_L_del_theta[0])\n",
    "    del_L_del_theta_sum = np.sum([del_L_del_theta[i] for i in U], axis=0)\n",
    "    hinv_del_L_del_theta= s_test(model, X_train, y_train, del_L_del_theta_sum, hinv=hinv)\n",
    "    hessian_U_hinv_del_L_del_theta = np.zeros((num_params,))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        x, y = torch.FloatTensor(X_train[idx]), torch.FloatTensor([y_train[idx]])\n",
    "        loss = loss_func(model, x, y)\n",
    "        params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "        hessian_U_hinv_del_L_del_theta += convert_grad_to_ndarray(hvp(loss, params, torch.FloatTensor(hinv_del_L_del_theta)))\n",
    "\n",
    "    term1 = c1 * hinv_del_L_del_theta\n",
    "    term2 = c2 * s_test(model, X_train, y_train, hessian_U_hinv_del_L_del_theta, hinv=hinv)\n",
    "    sum_term = term1 + term2\n",
    "    return sum_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.722278Z",
     "start_time": "2021-10-18T22:26:24.718509Z"
    }
   },
   "outputs": [],
   "source": [
    "def second_order_group_influence(U, del_L_del_theta):\n",
    "    u = len(U)\n",
    "    s = len(X_train)\n",
    "    p = u/s\n",
    "    c1 = (1 - 2*p)/(s * (1-p)**2)\n",
    "    c2 = 1/((s * (1-p))**2)\n",
    "    num_params = len(del_L_del_theta[0])\n",
    "    del_L_del_theta_sum = np.sum([del_L_del_theta[i] for i in U], axis=0)\n",
    "    hinv_del_L_del_theta= np.matmul(hinv, del_L_del_theta_sum)\n",
    "    hessian_U_hinv_del_L_del_theta = np.zeros((num_params,))\n",
    "    for i in range(u):\n",
    "        idx = U[i]\n",
    "        hessian_U_hinv_del_L_del_theta += np.matmul(hessian_all_points[idx], hinv_del_L_del_theta)\n",
    "\n",
    "    term1 = c1 * hinv_del_L_del_theta\n",
    "    term2 = c2 * np.matmul(hinv, hessian_U_hinv_del_L_del_theta)\n",
    "    sum_term = (term1 + term2*len(X_train))\n",
    "    return sum_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First-order influence of each training data point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.730333Z",
     "start_time": "2021-10-18T22:26:24.724605Z"
    }
   },
   "outputs": [],
   "source": [
    "infs_1 = first_order_influence(del_L_del_theta, hinv_v, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fairness: Ground-truth subset influence vs. computed subset influences: Coherent subset** \n",
    "\n",
    "(by coherent, we mean group of data points that share some properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:*** The retraining of the clf would cause the change in model parameters and thus lead to the change of gradients, so in this part, we first acquire all the first- and second-order influence functions together based on the original model. After all the influence functions are calculated, we retrain the model corresponding to different removed coherent subset of data and get the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.733486Z",
     "start_time": "2021-10-18T22:26:24.731361Z"
    }
   },
   "outputs": [],
   "source": [
    "time_gt = []\n",
    "time_first = []\n",
    "time_second = []\n",
    "rep = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.738878Z",
     "start_time": "2021-10-18T22:26:24.734751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_f_lower: 0.0009443411026149984\n",
      "alpha_f_upper: 0.09443411026149984\n",
      "del_f_threshold: -0.009443411026149984\n",
      "support_small: 0.3\n",
      "del_f_threshold_small: 0.009443411026149984\n"
     ]
    }
   ],
   "source": [
    "alpha_f_lower = (-0.01) * (spd_0)\n",
    "alpha_f_upper = -spd_0\n",
    "del_f_threshold = (0.1) * spd_0\n",
    "support = 0.05 # Do not consider extremely small patterns\n",
    "support_small = 0.3 # For small patterns, 2nd-order estimation is quite accurate\n",
    "del_f_threshold_small = (-0.1) * (spd_0)\n",
    "print(\"alpha_f_lower:\", alpha_f_lower)\n",
    "print(\"alpha_f_upper:\", alpha_f_upper)\n",
    "print(\"del_f_threshold:\", del_f_threshold)\n",
    "print(\"support_small:\", support_small)\n",
    "print(\"del_f_threshold_small:\", del_f_threshold_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.874906Z",
     "start_time": "2021-10-18T22:26:24.740317Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status: 3\n",
      "status: 2\n",
      "status: 0\n",
      "status: 1\n",
      "duration: 1\n",
      "duration: 0\n",
      "duration: 2\n",
      "duration: 3\n",
      "credit_hist: 2\n",
      "credit_hist: 0\n",
      "credit_hist: 4\n",
      "credit_hist: 3\n",
      "credit_hist: 1\n",
      "credit_amt: 0\n",
      "credit_amt: 1\n",
      "credit_amt: 2\n",
      "savings: 4\n",
      "savings: 1\n",
      "savings: 2\n",
      "savings: 0\n",
      "savings: 3\n",
      "employment: 3\n",
      "employment: 4\n",
      "employment: 2\n",
      "employment: 0\n",
      "employment: 1\n",
      "install_rate: 4\n",
      "install_rate: 1\n",
      "install_rate: 2\n",
      "install_rate: 3\n",
      "debtors: 0\n",
      "debtors: 2\n",
      "debtors: 1\n",
      "residence: 4\n",
      "residence: 3\n",
      "residence: 1\n",
      "residence: 2\n",
      "property: 2\n",
      "property: 1\n",
      "property: 3\n",
      "property: 0\n",
      "age: 0\n",
      "age: 1\n",
      "install_plans: 0\n",
      "install_plans: 1\n",
      "num_credits: 1\n",
      "num_credits: 2\n",
      "num_credits: 3\n",
      "num_credits: 4\n",
      "job: 2\n",
      "job: 3\n",
      "job: 0\n",
      "job: 1\n",
      "num_liable: 1\n",
      "num_liable: 2\n",
      "telephone: 0\n",
      "telephone: 1\n",
      "foreign_worker: 1\n",
      "foreign_worker: 0\n",
      "gender: 0\n",
      "gender: 1\n",
      "purpose_A40: 1\n",
      "purpose_A41: 1\n",
      "purpose_A410: 1\n",
      "purpose_A42: 1\n",
      "purpose_A43: 1\n",
      "purpose_A44: 1\n",
      "purpose_A45: 1\n",
      "purpose_A46: 1\n",
      "purpose_A48: 1\n",
      "purpose_A49: 1\n",
      "housing_A151: 1\n",
      "housing_A152: 1\n",
      "housing_A153: 1\n"
     ]
    }
   ],
   "source": [
    "for _ in range(rep):\n",
    "    # Get the original model\n",
    "#     clf = NeuralNetwork(input_size=X_train.shape[-1])\n",
    "    clf = LogisticRegression(input_size=X_train.shape[-1])\n",
    "    # clf = SVM()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    attributes = []\n",
    "    attributeValues = []\n",
    "    first_order_influences = []\n",
    "    second_order_influences = []\n",
    "    fractionRows = []\n",
    "\n",
    "    # print(\"Attribute, Value, Ground-truth subset, Add 1st-order inf individual, \\\n",
    "    # Second-order subset influence, %rowsRemoved, Accuracy\")\n",
    "    # clf.fit(X_train, y_train)\n",
    "    # continuous_cols = ['duration', 'credit_amt', 'install_rate', 'num_credits', 'residence']\n",
    "    v1_orig = v1\n",
    "    for col in X_train_orig.columns:\n",
    "        if dataset == 'german':\n",
    "            if \"purpose\" in col or \"housing\" in col: #dummy variables purpose=0 doesn't make sense\n",
    "                vals = [1]\n",
    "            else:\n",
    "                vals = X_train_orig[col].unique()\n",
    "        elif dataset == 'adult':\n",
    "            vals = X_train_orig[col].unique()\n",
    "        elif dataset == 'compas':\n",
    "            vals = X_train_orig[col].unique()\n",
    "        elif dataset == 'sqf':\n",
    "            vals = X_train_orig[col].unique()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        for val in vals:\n",
    "            print(col, val, sep=\": \")\n",
    "            idx = X_train_orig[X_train_orig[col] == val].index  \n",
    "            if (len(idx)/len(X_train) > support):  \n",
    "                X = np.delete(X_train, idx, 0)\n",
    "                y = y_train.drop(index=idx, inplace=False)\n",
    "                if len(y.unique()) > 1:\n",
    "                    idx = X_train_orig[X_train_orig[col] == val].index \n",
    "\n",
    "                    # First-order subset influence\n",
    "                    t0 = time.time()\n",
    "                    del_f_1 = 0            \n",
    "        #             for i in range(len(idx)):\n",
    "        #                 del_f_1 += infs_1[idx[i]]\n",
    "                   \n",
    "#                     params_f_1 = first_order_group_influence(idx, del_L_del_theta)\n",
    "#                     del_f_1 = np.dot(v1.transpose(), params_f_1)\n",
    "#                     time_first.append(time.time()-t0)\n",
    "\n",
    "                    # Second-order subset influence\n",
    "                    t0 = time.time()\n",
    "#                     print(col, val)\n",
    "#                     params_f_2 = second_order_influence(clf, X_train, y_train, idx, del_L_del_theta)\n",
    "#                     print(col, val)\n",
    "                    params_f_2 = second_order_group_influence(idx, del_L_del_theta)\n",
    "                    del_f_2 = np.dot(v1.transpose(), params_f_2)\n",
    "                    time_second.append(time.time()-t0)\n",
    "\n",
    "                    attributes.append(col)\n",
    "                    attributeValues.append(val)\n",
    "                    first_order_influences.append(del_f_1)\n",
    "                    second_order_influences.append(del_f_2)\n",
    "            #         gt_influences.append(inf_gt)\n",
    "                    fractionRows.append(len(idx)/len(X_train)*100)\n",
    "\n",
    "            #         print(col, val, inf_gt, del_f_1, del_f_2, len(idx)/len(X_train), accuracy, sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:24.878422Z",
     "start_time": "2021-10-18T22:26:24.875983Z"
    }
   },
   "outputs": [],
   "source": [
    "# for _ in range(rep):\n",
    "#     gt_influences = []\n",
    "# #     v1s = []\n",
    "#     for col in X_train_orig.columns:\n",
    "#         if dataset == 'german':\n",
    "#             if \"purpose\" in col or \"housing\" in col: #dummy variables purpose=0 doesn't make sense\n",
    "#                 vals = [1]\n",
    "#             else:\n",
    "#                 vals = X_train_orig[col].unique()\n",
    "#         elif dataset == 'adult':\n",
    "#             continuous_cols = ['age', 'education.num', 'hours',]\n",
    "#             if col in continuous_cols:\n",
    "#                 vals = X_train_orig[col].unique()\n",
    "#             else:\n",
    "#                 vals = [1]\n",
    "#         elif dataset == 'compas':\n",
    "#             vals = X_train_orig[col].unique()\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "#         for val in vals:\n",
    "#             print(col, val)\n",
    "#             idx = X_train_orig[X_train_orig[col] == val].index \n",
    "#             X = np.delete(X_train, idx, 0)\n",
    "#             y = y_train.drop(index=idx, inplace=False)\n",
    "#             inf_gt = 0\n",
    "#             if len(y.unique()) > 1:\n",
    "#                 # Ground truth subset influence\n",
    "#                 t0 = time.time()\n",
    "#                 clf.fit(np.array(X), np.array(y))\n",
    "#                 y_pred = clf.predict_proba(np.array(X_test))\n",
    "#                 if metric == 0:\n",
    "#                     inf_gt = computeFairness(y_pred, X_test_orig, y_test, 0, dataset) - spd_0\n",
    "#                 elif metric == 1:\n",
    "#                     inf_gt = computeFairness(y_pred, X_test_orig, y_test, 1, dataset) - tpr_parity_0\n",
    "#                 elif metric == 2:\n",
    "#                     inf_gt = computeFairness(y_pred, X_test_orig, y_test, 2, dataset) - predictive_parity_0\n",
    "#                 time_gt.append(time.time()-t0)\n",
    "#                 accuracy = computeAccuracy(y_test, y_pred)\n",
    "#                 gt_influences.append(inf_gt)\n",
    "# #                 v1s.append(del_spd_del_theta(clf, X_test_orig, X_test, dataset))  # here, v1 corresponds to the metric spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:25.302960Z",
     "start_time": "2021-10-18T22:26:25.297588Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# def compute_cosine_theta(vec1: np.ndarray, vec2: np.ndarray):\n",
    "#     \"\"\"\n",
    "#         Compute cosine of the angle between two vectors \n",
    "#     \"\"\"\n",
    "#     vec1 = vec1.squeeze()\n",
    "#     vec2 = vec2.squeeze()\n",
    "#     assert vec1.shape == vec2.shape\n",
    "#     vec1 /= np.sqrt(np.sum(vec1**2))\n",
    "#     vec2 /= np.sqrt(np.sum(vec2**2))\n",
    "#     return np.dot(vec1.T, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:25.586316Z",
     "start_time": "2021-10-18T22:26:25.581085Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# def compute_theta(vec1: np.ndarray, vec2: np.ndarray, measure='radian'):\n",
    "#     \"\"\"\n",
    "#         Compute the angle between two vectors (angle/radian measure)\n",
    "#     \"\"\"\n",
    "#     cos = compute_cosine_theta(vec1, vec2)\n",
    "#     theta = np.arccos(cos)\n",
    "#     if measure == 'angle':\n",
    "#         theta = theta / np.pi * 180\n",
    "#     return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:26.101508Z",
     "start_time": "2021-10-18T22:26:26.096143Z"
    }
   },
   "outputs": [],
   "source": [
    "# cos_thetas = []\n",
    "# thetas = []\n",
    "# grad_change_scales = []\n",
    "# for vec in v1s:\n",
    "#     cos_thetas.append(compute_cosine_theta(v1_orig, vec))\n",
    "#     thetas.append(compute_theta(v1_orig, vec, measure='angle'))\n",
    "#     grad_change_scales.append(np.sqrt(np.sum((vec-v1_orig)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:26.569711Z",
     "start_time": "2021-10-18T22:26:26.551803Z"
    }
   },
   "outputs": [],
   "source": [
    "# expl = [attributes, attributeValues, first_order_influences, second_order_influences, gt_influences, fractionRows, thetas, grad_change_scales]\n",
    "# expl = (np.array(expl).T).tolist()\n",
    "\n",
    "# explanations = pd.DataFrame(expl, columns=[\"attributes\", \"attributeValues\", \"first_order_influences\", \"second_order_influences\", \"gt_influences\", \"fractionRows\", \"gradient_angles\", \"grad_change_scales\"])\n",
    "\n",
    "expl = [attributes, attributeValues, first_order_influences, second_order_influences, fractionRows]\n",
    "expl = (np.array(expl).T).tolist()\n",
    "\n",
    "explanations = pd.DataFrame(expl, columns=[\"attributes\", \"attributeValues\", \"first_order_influences\", \"second_order_influences\", \"fractionRows\"])\n",
    "explanations['second_order_influences'] = explanations['second_order_influences'].astype(float)\n",
    "explanations['first_order_influences'] = explanations['first_order_influences'].astype(float)\n",
    "# explanations['gt_influences'] = explanations['gt_influences'].astype(float)\n",
    "explanations['fractionRows'] = explanations['fractionRows'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:30.281965Z",
     "start_time": "2021-10-18T22:26:30.270357Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates = explanations[(explanations[\"second_order_influences\"] > alpha_f_lower) \n",
    "                           & (explanations[\"second_order_influences\"] < alpha_f_upper)]\n",
    "candidates = copy.deepcopy(explanations)\n",
    "candidates.loc[:, 'score'] = candidates.loc[:, 'second_order_influences']*100/candidates.loc[:, 'fractionRows']\n",
    "# display(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:26:30.760815Z",
     "start_time": "2021-10-18T22:26:30.751794Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:06:31.175126Z",
     "start_time": "2021-10-18T23:06:31.153903Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['age=0'], 79.875, 0.0007946174454818204, 0.0006347006845786041],\n",
       " [['age=1'], 20.125, 0.18119060530210618, 0.03646460931704887],\n",
       " [['credit_amt=0'], 44.0, 0.1248494452198182, 0.054933755896720005],\n",
       " [['credit_amt=1'], 38.125, 0.041956276999903855, 0.015995830606213346],\n",
       " [['credit_amt=2'], 17.875, -0.1561885835174065, -0.02791870930373641],\n",
       " [['credit_hist=0'],\n",
       "  28.749999999999996,\n",
       "  0.11212033663576179,\n",
       "  0.03223459678278151],\n",
       " [['credit_hist=1'], 9.25, -0.23324339781365805, -0.021575014297763373],\n",
       " [['credit_hist=2'], 53.0, 0.033784791203993116, 0.01790593933811635],\n",
       " [['debtors=0'], 90.625, 0.09456454311080521, 0.08569911719416723],\n",
       " [['debtors=2'], 5.25, 0.298012028937769, 0.015645631519232872],\n",
       " [['duration=0'], 35.125, 0.06108841264011355, 0.021457304939839883],\n",
       " [['duration=1'], 42.375, 0.004039442485628696, 0.00171171375328516],\n",
       " [['duration=2'],\n",
       "  14.249999999999998,\n",
       "  -0.09802046489294446,\n",
       "  -0.013967916247244583],\n",
       " [['duration=3'], 8.25, 0.11620102496884176, 0.009586584559929445],\n",
       " [['employment=0'], 6.75, 0.37777586067185803, 0.025499870595350416],\n",
       " [['employment=1'], 16.5, 0.044065324204427145, 0.007270778493730479],\n",
       " [['employment=2'], 34.375, -0.03428535962750733, -0.011785592371955644],\n",
       " [['employment=3'], 17.125, 0.05158407308942435, 0.00883377251656392],\n",
       " [['employment=4'], 25.25, -0.08917837448230247, -0.022517539556781374],\n",
       " [['foreign_worker=1'], 96.25, 0.5488493071532078, 0.5282674581349625],\n",
       " [['gender=0'], 32.125, 0.19810795471464568, 0.06364218045207992],\n",
       " [['gender=1'], 67.875, -0.16388945482660416, -0.11123996746355758],\n",
       " [['housing_A151=1'], 19.0, 0.06636883005047878, 0.012610077709590967],\n",
       " [['housing_A152=1'], 70.0, 0.06628572183041724, 0.04640000528129207],\n",
       " [['housing_A153=1'], 11.0, -0.23051576291994724, -0.025356733921194197],\n",
       " [['install_plans=0'], 80.5, 0.05828856854498417, 0.046922297678712255],\n",
       " [['install_plans=1'], 19.5, 0.19657203559640216, 0.03833154694129842],\n",
       " [['install_rate=1'],\n",
       "  13.875000000000002,\n",
       "  -0.22424481194829535,\n",
       "  -0.03111396765782598],\n",
       " [['install_rate=2'], 23.125, 0.06271211225052835, 0.014502175957934682],\n",
       " [['install_rate=3'], 15.375, 0.08473002147583221, 0.013027240801909201],\n",
       " [['install_rate=4'], 47.625, 0.10677359431911278, 0.05085092429447746],\n",
       " [['job=1'], 19.0, -0.06139458979047084, -0.01166497206018946],\n",
       " [['job=2'], 63.625, 0.033972725882498174, 0.021615146842739463],\n",
       " [['job=3'], 15.125, 0.1860109936210736, 0.028134162785187378],\n",
       " [['num_credits=1'], 63.125, 0.12849123321338352, 0.08111009096594834],\n",
       " [['num_credits=2'], 33.75, -0.026752508738407295, -0.009028971699212462],\n",
       " [['num_liable=1'], 85.5, 0.021204411714361833, 0.018129772015779366],\n",
       " [['num_liable=2'],\n",
       "  14.499999999999998,\n",
       "  -0.18671417045175398,\n",
       "  -0.027073554715504322],\n",
       " [['property=0'], 15.75, -0.05102370983641024, -0.008036234299234613],\n",
       " [['property=1'], 34.0, 0.07288693136856213, 0.024781556665311122],\n",
       " [['property=2'], 22.125, 0.05010405574589764, 0.011085522333779851],\n",
       " [['property=3'], 28.125, 0.009641978957296892, 0.0027118065817397508],\n",
       " [['purpose_A40=1'], 24.5, 0.01980903648204122, 0.0048532139381001],\n",
       " [['purpose_A41=1'], 10.75, -0.049483527661740176, -0.005319479223637069],\n",
       " [['purpose_A42=1'], 18.125, 0.09081212140141279, 0.01645969700400607],\n",
       " [['purpose_A43=1'], 28.125, 0.08847720592468594, 0.02488421416631792],\n",
       " [['purpose_A49=1'], 9.375, -0.1697815755715674, -0.015917022709834444],\n",
       " [['residence=1'], 12.25, -0.009893161566792532, -0.0012119122919320852],\n",
       " [['residence=2'],\n",
       "  31.125000000000004,\n",
       "  0.16194234061019688,\n",
       "  0.05040455351492379],\n",
       " [['residence=3'], 15.75, -0.07559516988342029, -0.011906239256638695],\n",
       " [['residence=4'], 40.875, -0.014204775340750106, -0.005806201920531606],\n",
       " [['savings=0'], 60.25, 0.152588025959494, 0.09193428564059514],\n",
       " [['savings=1'], 9.625, -0.2571277104210926, -0.02474854212803016],\n",
       " [['savings=2'], 6.25, 0.07074829574571417, 0.004421768484107136],\n",
       " [['savings=3'], 5.125, -0.1726725380992083, -0.008849467577584424],\n",
       " [['savings=4'], 18.75, 0.041420831946166935, 0.0077664059899063],\n",
       " [['status=0'], 27.625, 0.033840240953571736, 0.009348366563424193],\n",
       " [['status=1'], 27.125, -0.02137433978462247, -0.005797789666578845],\n",
       " [['status=2'], 6.125, 0.0798279822714346, 0.004889463914125369],\n",
       " [['status=3'], 39.125, 0.037617390229543295, 0.014717803927308815],\n",
       " [['telephone=0'], 60.375, -0.0008577634540080425, -0.0005178746853573557],\n",
       " [['telephone=1'], 39.625, 0.11319100659811993, 0.04485193636450502]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:19:47.377187Z",
     "start_time": "2021-10-18T23:17:34.340234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:  62  1-candidates\n",
      "Generated:  342  2-candidates\n",
      "Generated:  2\n",
      "Generated: 2432   3 -candidates\n",
      "Generated:  3\n",
      "Generated: 11460   4 -candidates\n",
      "CPU times: user 2min 12s, sys: 214 ms, total: 2min 13s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "candidates_all = []\n",
    "\n",
    "# Generating 1-candidates\n",
    "candidates_1 = []\n",
    "for i in range(len(candidates)):\n",
    "    candidate = []\n",
    "    candidate_i = candidates.iloc[i]\n",
    "#     if ((candidate_i[\"second_order_influences\"] > del_f_threshold) & \n",
    "#         (candidate_i[\"fractionRows\"] > support)):\n",
    "    if ((candidate_i[\"fractionRows\"] >= support_small) or\n",
    "       ((candidate_i[\"fractionRows\"] >= support) & (candidate_i[\"second_order_influences\"] > del_f_threshold))\n",
    "       ):\n",
    "        attr_i = candidate_i[\"attributes\"]\n",
    "        val_i = str(candidate_i[\"attributeValues\"])\n",
    "        predicates = []\n",
    "        predicates.insert(0, attr_i + '=' + str(val_i))\n",
    "        candidate.insert(0, predicates)\n",
    "        candidate.insert(1, candidate_i[\"fractionRows\"])\n",
    "        candidate.insert(2, candidate_i[\"score\"])\n",
    "        candidate.insert(3, candidate_i[\"second_order_influences\"])\n",
    "        candidates_1.insert(i, candidate)\n",
    "\n",
    "print(\"Generated: \", len(candidates_1), \" 1-candidates\")\n",
    "candidates_1.sort()\n",
    "# display(candidates_1)\n",
    "\n",
    "for i in range(len(candidates_1)):\n",
    "    if (float(candidates_1[i][2]) >= support): # if score > top-k, keep in candidates, not otherwise\n",
    "        candidates_all.insert(len(candidates_all), candidates_1[i])\n",
    "    \n",
    "# Generating 2-candidates\n",
    "candidates_2 = []\n",
    "for i in range(len(candidates_1)):\n",
    "    attr_i = candidates_1[i][0][0].split(\"=\")[0]\n",
    "    val_i = int(float(candidates_1[i][0][0].split(\"=\")[1]))\n",
    "    for j in range(i):\n",
    "        # merge two candidates\n",
    "        attr_j = candidates_1[j][0][0].split(\"=\")[0]\n",
    "        val_j = int(float(candidates_1[j][0][0].split(\"=\")[1]))\n",
    "        \n",
    "        if (attr_i != attr_j):\n",
    "            idx = X_train_orig[(X_train_orig[attr_i] == val_i) &\n",
    "                              (X_train_orig[attr_j] == val_j)].index \n",
    "            \n",
    "            idx_i = X_train_orig[(X_train_orig[attr_i] == val_i)].index \n",
    "            idx_j = X_train_orig[(X_train_orig[attr_j] == val_j)].index \n",
    "            fractionRows = len(idx)/len(X_train) * 100\n",
    "            isCompact = True\n",
    "            if (len(idx) == min(len(idx_i), len(idx_j))): # pattern is not compact if intersection equals one of its parents\n",
    "                isCompact = False\n",
    "            if (fractionRows/100 >= support):\n",
    "                X = np.delete(X_train, idx, 0)\n",
    "                y = y_train.drop(index=idx, inplace=False)\n",
    "\n",
    "                size_hvp = 1\n",
    "                params_f_2 = second_order_group_influence(idx, del_L_del_theta)\n",
    "                del_f_2 = np.dot(v1.transpose(), params_f_2)\n",
    "                    \n",
    "#                 params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "#                 del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "                \n",
    "                score = del_f_2 * 100/fractionRows\n",
    "\n",
    "                if (((score > candidates_1[i][2]) & (score > candidates_1[j][2]) \n",
    "#                      & (del_f_2 > del_f_threshold_small)\n",
    "                    )\n",
    "                   or (fractionRows/100 >= support_small)):\n",
    "                        candidate = []\n",
    "                        predicates = []\n",
    "                        predicates.insert(0, attr_i + '=' + str(val_i))\n",
    "                        predicates.insert(1, attr_j + '=' + str(val_j))\n",
    "                        candidate.insert(0, sorted(predicates, key=itemgetter(0)))                        \n",
    "                        candidate.insert(1, len(idx)*100/len(X_train))\n",
    "                        candidate.insert(2, score)\n",
    "                        candidate.insert(3, del_f_2)\n",
    "                        candidates_2.insert(len(candidates_2), candidate)  \n",
    "                        if (isCompact):\n",
    "                            candidates_all.insert(len(candidates_all), candidate)\n",
    "print(\"Generated: \", len(candidates_2), \" 2-candidates\")\n",
    "candidates_2.sort()\n",
    "\n",
    "# Recursively generating the rest\n",
    "candidates_L_1 = copy.deepcopy(candidates_2)\n",
    "iter=2\n",
    "while((len(candidates_L_1) > 0) & (iter < 4)):\n",
    "    print(\"Generated: \", iter)    \n",
    "    candidates_L = []\n",
    "    for i in range(len(candidates_L_1)):\n",
    "        candidate_i = candidates_L_1[i][0]\n",
    "        for j in range(i):\n",
    "            candidate_j = candidates_L_1[j][0]\n",
    "            # if L-1 lists intersect\n",
    "            intersect_candidates = set(candidate_i).intersection(candidate_j)\n",
    "            if (len(intersect_candidates) == iter - 1):\n",
    "                setminus_i = list(set(candidate_i) - intersect_candidates)[0].split(\"=\")\n",
    "                setminus_j = list(set(candidate_j) - intersect_candidates)[0].split(\"=\")\n",
    "                attr_i = setminus_i[0]\n",
    "                val_i = int(setminus_i[1])\n",
    "                attr_j = setminus_j[0]\n",
    "                val_j = int(setminus_j[1])\n",
    "                if (attr_i != attr_j):\n",
    "                    # merge to get L list\n",
    "                    merged_candidate = list(set(candidate_i + candidate_j))\n",
    "\n",
    "                    idx_i_j = pd.Index(list(range(len(X_train_orig))))\n",
    "                    for k in range(len(intersect_candidates)):\n",
    "                        attr = list(intersect_candidates)[k].split(\"=\")[0]\n",
    "                        val = int(float(list(intersect_candidates)[k].split(\"=\")[1]))\n",
    "                        idx_i_j = idx_i_j.intersection(X_train_orig[(X_train_orig[attr] == val)].index)\n",
    "                    \n",
    "                    idx_i = idx_i_j.intersection(X_train_orig[(X_train_orig[attr_i] == val_i)].index)\n",
    "                    idx_j = idx_i_j.intersection(X_train_orig[(X_train_orig[attr_j] == val_j)].index)                    \n",
    "                    idx = idx_i.intersection(X_train_orig[(X_train_orig[attr_j] == val_j)].index) # merged\n",
    "\n",
    "                    fractionRows = len(idx)/len(X_train) * 100\n",
    "                    isCompact = True\n",
    "                    if (len(idx) == min(len(idx_i), len(idx_j))): # pattern is not compact if intersection equals one of its parents\n",
    "                        isCompact = False\n",
    "                    if (fractionRows/100 >= support):\n",
    "                        X = np.delete(X_train, idx, 0)\n",
    "                        y = y_train.drop(index=idx, inplace=False)\n",
    "\n",
    "                        size_hvp = 1\n",
    "                        params_f_2 = second_order_group_influence(idx, del_L_del_theta)\n",
    "                        del_f_2 = np.dot(v1.transpose(), params_f_2)\n",
    "                    \n",
    "#                         params_f_2 = second_order_influence(X_train, idx, size_hvp, del_L_del_theta, hessian_all_points)\n",
    "#                         del_f_2 = np.dot(v1.transpose(), params_f_2)[0][0]\n",
    "    \n",
    "                        score = del_f_2 * 100/fractionRows\n",
    "                        if (((score > candidates_L_1[i][2]) & (score > candidates_L_1[j][2]) \n",
    "#                              & (del_f_2 > del_f_threshold_small)\n",
    "                            ) or \n",
    "                           (fractionRows >= support_small)):\n",
    "                            candidate = []\n",
    "                            candidate.insert(0, sorted(merged_candidate, key=itemgetter(0)))                        \n",
    "                            candidate.insert(1, fractionRows)\n",
    "                            candidate.insert(2, del_f_2*len(X_train)/len(idx))\n",
    "                            candidate.insert(3, del_f_2)\n",
    "                            if (candidate not in candidates_L):\n",
    "                                candidates_L.insert(len(candidates_L), candidate)\n",
    "                                if (isCompact):\n",
    "                                    candidates_all.insert(len(candidates_all), candidate)\n",
    "\n",
    "    print(\"Generated:\", len(candidates_L), \" \", str(iter+1), \"-candidates\")\n",
    "    candidates_L_1 = copy.deepcopy(candidates_L)\n",
    "    candidates_L_1.sort()\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:25:11.212197Z",
     "start_time": "2021-10-18T23:25:11.070435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14085\n",
      "14085\n"
     ]
    }
   ],
   "source": [
    "candidates_support_3_compact = copy.deepcopy(candidates_all)\n",
    "print(len(candidates_support_3_compact))\n",
    "candidates_df_3_compact = pd.DataFrame(candidates_support_3_compact, columns=[\"predicates\",\"support\",\"score\",\"2nd-inf\"])\n",
    "candidates_df_3_compact = candidates_df_3_compact.sort_values(by=['score'], ascending=False)\n",
    "print(len(candidates_df_3_compact))\n",
    "# display(candidates_df_3_compact)\n",
    "# display(candidates_df_3_compact[candidates_df_3_compact[\"support\"] < 20].sort_values(by=['2nd-inf'], ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:25:12.599775Z",
     "start_time": "2021-10-18T23:25:12.596847Z"
    }
   },
   "outputs": [],
   "source": [
    "# txt = json.dumps(candidates_all)\n",
    "# with open('candidates_all.bak', 'w+') as f:\n",
    "#     f.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Containment-based filtering (with LSH Ensemble)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:33:20.747066Z",
     "start_time": "2021-10-18T23:32:43.507888Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_subset(X_train_orig, explanation):\n",
    "    subset = X_train_orig.copy()\n",
    "    for predicate in explanation:\n",
    "#         print(predicate)\n",
    "        attr = predicate.split(\"=\")[0].strip(' ')\n",
    "        val = int(predicate.split(\"=\")[1].strip(' '))\n",
    "        subset = subset[subset[attr]==val]\n",
    "    return subset.index\n",
    "\n",
    "from datasketch import MinHashLSHEnsemble, MinHash, MinHashLSH\n",
    "import json\n",
    "\n",
    "class Topk:\n",
    "    '''\n",
    "        top explanations: explanation -> (minhash, set_index, score)\n",
    "    '''\n",
    "    def __init__(self, method='lsh', init_df=X_test_orig, init_explanations=[], threshold=0.75, k=5, num_perm=128):\n",
    "        self.method = method\n",
    "        self.num_perm = num_perm\n",
    "        if method == 'lshensemble':\n",
    "#             self.index = MinHashLSHEnsemble(threshold=threshold, num_perm=num_perm)\n",
    "#             hashed_explanations = []\n",
    "#             for explanation, score in init_explanations:\n",
    "#                 s = get_subset(init_df, explanation)\n",
    "#                 m = MinHash(num_perm=num_perm)\n",
    "#                 explanation_json = json.dumps(explanation)\n",
    "#                 for d in s:\n",
    "#                     m.update(str(d).encode('utf8'))\n",
    "#                 hashed_explanations.append((explanation_json, m, len(s)))\n",
    "#             self.index.index(hashed_explanations)\n",
    "            raise NotImplementedError\n",
    "        elif method == 'lsh':\n",
    "#             self.index = MinHashLSH(threshold=threshold, num_perm=num_perm, params=[64, self.num_perm//64])\n",
    "#             for explanation, score in init_explanations:\n",
    "#                 s = get_subset(init_df, explanation)\n",
    "#                 m = MinHash(num_perm=num_perm)\n",
    "#                 explanation_json = json.dumps(explanation)\n",
    "#                 for d in s:\n",
    "#                     m.update(str(d).encode('utf8'))\n",
    "#                 self.index.insert(explanation_json, m)\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.top_explanations = dict()\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "        self.min_score = -100\n",
    "        self.min_score_explanation =None\n",
    "        self.containment_hist = []\n",
    "    \n",
    "    def _update_min(self, new_explanation, new_score):\n",
    "        if len(self.top_explanations) > 0:\n",
    "            for explanation, t in self.top_explanations.items():\n",
    "                if t[2] < new_score:\n",
    "                    new_score = t[2]\n",
    "                    new_explanation = explanation\n",
    "        self.min_score = new_score\n",
    "        self.min_score_explanation = new_explanation\n",
    "        \n",
    "    def _containment(self, x, q):\n",
    "        c = len(x & q)/len(q)\n",
    "        self.containment_hist.append(c)\n",
    "        return c\n",
    "            \n",
    "    def update(self, df, explanation, score):\n",
    "        if (len(self.top_explanations) < self.k) or (score > self.min_score):\n",
    "            s = get_subset(df, explanation)\n",
    "            m = MinHash(num_perm=self.num_perm)\n",
    "            explanation = json.dumps(explanation)\n",
    "            for d in s:\n",
    "                m.update(str(d).encode('utf8'))\n",
    "\n",
    "            if self.method == 'lshensemble':\n",
    "#                 q_result = set(self.index.query(m, len(s))).intersection(set(self.top_explanations.keys()))\n",
    "                raise NotImplementedError\n",
    "            elif self.method == 'lsh':\n",
    "#                 q_result = set(self.index.query(m)).intersection(set(self.top_explanations.keys()))\n",
    "                raise NotImplementedError\n",
    "            elif self.method == 'containment':\n",
    "                q_result = set()\n",
    "                for k, v in self.top_explanations.items():\n",
    "                    if self._containment(v[1], s) > self.threshold:\n",
    "#                         print(self._containment(v[1], s))\n",
    "                        q_result.add(k)\n",
    "            \n",
    "#             print(q_result)\n",
    "#             print([round(len(self.top_explanations[q][1] & s)/len(s), 3) for q in q_result])\n",
    "#             print([round(len(self.top_explanations[q][1] & s)/len(self.top_explanations[q][1] | s), 3) for q in self.top_explanations.keys()])\n",
    "\n",
    "            if len(q_result)==0:\n",
    "                if len(self.top_explanations) <= self.k-1:\n",
    "                    self._update_min(explanation, score)\n",
    "                    self.top_explanations[explanation] = (m, s, score)\n",
    "                    return 0\n",
    "#                 else:\n",
    "#                     del self.top_explanations[self.min_score_explanation]\n",
    "#                     self._update_min(explanation, score)\n",
    "#                     self.top_explanations[explanation] = (m, s, score)\n",
    "#                     return 0\n",
    "#             else:\n",
    "#                 q_scores = [self.top_explanations[explanation][2] for explanation in q_result]\n",
    "#                 if max(q_scores) < score:\n",
    "#                     for explanation in q_result:\n",
    "#                         del self.top_explanations[explanation]\n",
    "#                     self._update_min(explanation, score)\n",
    "#                     self.top_explanations[explanation] = (m, s, score)\n",
    "# #                     print('Inserted')\n",
    "#                     return 0\n",
    "        return -1\n",
    "\n",
    "lsh_df = candidates_df_3_compact.sort_values(by=['score'], ascending=False).copy()\n",
    "\n",
    "topk = Topk(method='containment', threshold=0.3, k=5)\n",
    "for row_idx in range(len(lsh_df)):\n",
    "    row = lsh_df.iloc[row_idx]\n",
    "    explanation, score = row[0], row[2]\n",
    "    topk.update(X_train_orig, explanation, score)\n",
    "    if len(topk.top_explanations) == topk.k:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:33:39.708731Z",
     "start_time": "2021-10-18T23:33:39.700362Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[\"age=1\", \"gender=0\"]': (<datasketch.minhash.MinHash at 0x17281c2e0>,\n",
       "  Int64Index([ 13,  37,  76, 196, 212, 233, 241, 266, 285, 294, 333, 342, 354,\n",
       "              357, 370, 376, 398, 430, 435, 444, 494, 526, 537, 542, 545, 553,\n",
       "              562, 567, 588, 592, 598, 608, 613, 656, 671, 701, 727, 737, 749,\n",
       "              797],\n",
       "             dtype='int64'),\n",
       "  1.1505784267666819),\n",
       " '[\"credit_hist=2\", \"credit_amt=0\", \"gender=0\", \"housing_A152=1\"]': (<datasketch.minhash.MinHash at 0x17281c1f0>,\n",
       "  Int64Index([  5,  27,  37,  39,  59,  81, 102, 160, 183, 191, 196, 204, 205,\n",
       "              241, 243, 245, 256, 258, 285, 297, 311, 333, 353, 354, 417, 419,\n",
       "              430, 434, 445, 446, 448, 449, 460, 489, 494, 497, 501, 507, 526,\n",
       "              560, 562, 567, 577, 588, 598, 605, 615, 629, 633, 637, 696, 701,\n",
       "              703, 718, 724],\n",
       "             dtype='int64'),\n",
       "  0.5906603564392345),\n",
       " '[\"age=1\", \"credit_amt=1\", \"num_liable=1\"]': (<datasketch.minhash.MinHash at 0x17281c7f0>,\n",
       "  Int64Index([  1,  63,  64, 146, 182, 208, 217, 233, 238, 252, 265, 266, 306,\n",
       "              337, 342, 376, 378, 393, 398, 399, 409, 444, 462, 487, 503, 537,\n",
       "              545, 604, 609, 613, 614, 623, 644, 656, 661, 669, 670, 671, 675,\n",
       "              690, 694, 707, 797],\n",
       "             dtype='int64'),\n",
       "  0.5806329662445108),\n",
       " '[\"foreign_worker=1\"]': (<datasketch.minhash.MinHash at 0x17281c580>,\n",
       "  Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
       "              ...\n",
       "              790, 791, 792, 793, 794, 795, 796, 797, 798, 799],\n",
       "             dtype='int64', length=770),\n",
       "  0.5488493071532078)}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk.top_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:33:45.569400Z",
     "start_time": "2021-10-18T23:33:45.430784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explanations</th>\n",
       "      <th>support</th>\n",
       "      <th>score</th>\n",
       "      <th>2nd-inf(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"age=1\", \"gender=0\"]</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.150578</td>\n",
       "      <td>0.609196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"credit_hist=2\", \"credit_amt=0\", \"gender=0\", \"housing_A152=1\"]</td>\n",
       "      <td>6.875</td>\n",
       "      <td>0.590660</td>\n",
       "      <td>0.430013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"age=1\", \"credit_amt=1\", \"num_liable=1\"]</td>\n",
       "      <td>5.375</td>\n",
       "      <td>0.580633</td>\n",
       "      <td>0.330485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"foreign_worker=1\"]</td>\n",
       "      <td>96.250</td>\n",
       "      <td>0.548849</td>\n",
       "      <td>5.594032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      explanations  support  \\\n",
       "0                                            [\"age=1\", \"gender=0\"]    5.000   \n",
       "1  [\"credit_hist=2\", \"credit_amt=0\", \"gender=0\", \"housing_A152=1\"]    6.875   \n",
       "2                        [\"age=1\", \"credit_amt=1\", \"num_liable=1\"]    5.375   \n",
       "3                                             [\"foreign_worker=1\"]   96.250   \n",
       "\n",
       "      score  2nd-inf(%)  \n",
       "0  1.150578    0.609196  \n",
       "1  0.590660    0.430013  \n",
       "2  0.580633    0.330485  \n",
       "3  0.548849    5.594032  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations = list(topk.top_explanations.keys())\n",
    "idxs = [v[1] for v in topk.top_explanations.values()]\n",
    "supports = list()\n",
    "scores = list()\n",
    "infs = list()\n",
    "for e in explanations:\n",
    "    condition = candidates_df_3_compact.predicates.apply(lambda x: x==json.loads(e))\n",
    "    supports.append(float(candidates_df_3_compact[condition]['support']))\n",
    "    scores.append(float(candidates_df_3_compact[condition]['score']))\n",
    "    infs.append(float(candidates_df_3_compact[condition]['2nd-inf']))\n",
    "\n",
    "expl = [explanations, supports, scores, infs]\n",
    "expl = (np.array(expl).T).tolist()\n",
    "\n",
    "explanations = pd.DataFrame(expl, columns=[\"explanations\", \"support\", \"score\", \"2nd-inf(%)\"])\n",
    "explanations['score'] = explanations['score'].astype(float)\n",
    "explanations['support'] = explanations['support'].astype(float)\n",
    "explanations['2nd-inf(%)'] = explanations['2nd-inf(%)'].astype(float)/(-spd_0)\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "explanations.sort_values(by=['score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:31:19.313801Z",
     "start_time": "2021-10-18T23:31:19.285841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicates</th>\n",
       "      <th>support</th>\n",
       "      <th>score</th>\n",
       "      <th>2nd-inf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>[age=1, gender=0]</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.150578</td>\n",
       "      <td>0.057529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>[credit_hist=2, credit_amt=0, gender=0, housing_A152=1]</td>\n",
       "      <td>6.875</td>\n",
       "      <td>0.590660</td>\n",
       "      <td>0.040608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>[age=1, credit_amt=1, num_liable=1]</td>\n",
       "      <td>5.375</td>\n",
       "      <td>0.580633</td>\n",
       "      <td>0.031209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>[age=1, credit_hist=2, foreign_worker=1, housing_A152=1]</td>\n",
       "      <td>5.250</td>\n",
       "      <td>0.564272</td>\n",
       "      <td>0.029624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123</th>\n",
       "      <td>[age=1, housing_A152=1, install_plans=0, job=2]</td>\n",
       "      <td>6.125</td>\n",
       "      <td>0.562695</td>\n",
       "      <td>0.034465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12514</th>\n",
       "      <td>[age=1, gender=1, install_plans=0, telephone=1]</td>\n",
       "      <td>5.375</td>\n",
       "      <td>-0.531270</td>\n",
       "      <td>-0.028556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12448</th>\n",
       "      <td>[gender=1, install_plans=0, job=2, num_liable=2]</td>\n",
       "      <td>5.250</td>\n",
       "      <td>-0.542444</td>\n",
       "      <td>-0.028478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>[age=1, gender=1, housing_A152=1, num_credits=1]</td>\n",
       "      <td>5.000</td>\n",
       "      <td>-0.587073</td>\n",
       "      <td>-0.029354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12463</th>\n",
       "      <td>[gender=1, housing_A153=1, install_plans=0, residence=4]</td>\n",
       "      <td>5.125</td>\n",
       "      <td>-0.632441</td>\n",
       "      <td>-0.032413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5143</th>\n",
       "      <td>[age=1, gender=1, install_plans=0, num_credits=1]</td>\n",
       "      <td>6.250</td>\n",
       "      <td>-0.637365</td>\n",
       "      <td>-0.039835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14085 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     predicates  support  \\\n",
       "77                                            [age=1, gender=0]    5.000   \n",
       "7048    [credit_hist=2, credit_amt=0, gender=0, housing_A152=1]    6.875   \n",
       "599                         [age=1, credit_amt=1, num_liable=1]    5.375   \n",
       "5075   [age=1, credit_hist=2, foreign_worker=1, housing_A152=1]    5.250   \n",
       "5123            [age=1, housing_A152=1, install_plans=0, job=2]    6.125   \n",
       "...                                                         ...      ...   \n",
       "12514           [age=1, gender=1, install_plans=0, telephone=1]    5.375   \n",
       "12448          [gender=1, install_plans=0, job=2, num_liable=2]    5.250   \n",
       "5101           [age=1, gender=1, housing_A152=1, num_credits=1]    5.000   \n",
       "12463  [gender=1, housing_A153=1, install_plans=0, residence=4]    5.125   \n",
       "5143          [age=1, gender=1, install_plans=0, num_credits=1]    6.250   \n",
       "\n",
       "          score   2nd-inf  \n",
       "77     1.150578  0.057529  \n",
       "7048   0.590660  0.040608  \n",
       "599    0.580633  0.031209  \n",
       "5075   0.564272  0.029624  \n",
       "5123   0.562695  0.034465  \n",
       "...         ...       ...  \n",
       "12514 -0.531270 -0.028556  \n",
       "12448 -0.542444 -0.028478  \n",
       "5101  -0.587073 -0.029354  \n",
       "12463 -0.632441 -0.032413  \n",
       "5143  -0.637365 -0.039835  \n",
       "\n",
       "[14085 rows x 4 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_df_3_compact.sort_values(by=['score'], ascending=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:28:11.363389Z",
     "start_time": "2021-10-18T23:28:11.349146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explanations</th>\n",
       "      <th>support</th>\n",
       "      <th>score</th>\n",
       "      <th>2nd-inf(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [explanations, support, score, 2nd-inf(%)]\n",
       "Index: []"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations[explanations['explanations']==json.dumps([\"gender=0\", \"credit_hist=2\", \"credit_amt=0\", \"housing_A152=1\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:22:14.464456Z",
     "start_time": "2021-10-18T23:22:14.437596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%rows 5.0\n",
      "inf-gt 0.42905241057478793\n"
     ]
    }
   ],
   "source": [
    "# idx = X_train_orig[\n",
    "#                    (X_train_orig['cs_casng']==1)\n",
    "#                    & (X_train_orig['cs_drgtr']==0)\n",
    "#                    & (X_train_orig['cs_vcrim']==0)\n",
    "#                    & (X_train_orig['race']==1)\n",
    "#                   ].index \n",
    "# idx = X_train_orig[\n",
    "#                    (X_train_orig['credit_hist']==2)\n",
    "#                    & (X_train_orig['credit_amt']==0)\n",
    "#                    & (X_train_orig['gender']==0)\n",
    "#                    & (X_train_orig['housing_A152']==1)\n",
    "#                   ].index \n",
    "idx = X_train_orig[(X_train_orig['gender']==0)\n",
    "                   & (X_train_orig['age']==1)\n",
    "                  ].index \n",
    "print(\"%rows\", len(idx)*100/len(X_train))\n",
    "X = np.delete(X_train, idx, 0)\n",
    "y = y_train.drop(index=idx, inplace=False)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "print(\"inf-gt\", 1-computeFairness(y_pred, X_test_orig, y_test, 0, dataset)/spd_0)\n",
    "# print(\"gt\", computeFairness(y_pred, X_test_orig, y_test, 0, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(computeFairness(y_pred, X_test_orig, y_test, 0, dataset))\n",
    "spd_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = list(topk.top_explanations.keys())\n",
    "idxs = [v[1] for v in topk.top_explanations.values()]\n",
    "supports = list()\n",
    "scores = list()\n",
    "infs = list()\n",
    "for e in explanations:\n",
    "    condition = candidates_df_3_compact.predicates.apply(lambda x: x==json.loads(e))\n",
    "    supports.append(float(candidates_df_3_compact[condition]['support']))\n",
    "    scores.append(float(candidates_df_3_compact[condition]['score']))\n",
    "    infs.append(float(candidates_df_3_compact[condition]['2nd-inf']))\n",
    "\n",
    "expl = [explanations, supports, scores, infs]\n",
    "expl = (np.array(expl).T).tolist()\n",
    "\n",
    "explanations = pd.DataFrame(expl, columns=[\"explanations\", \"support\", \"score\", \"2nd-inf\"])\n",
    "explanations['score'] = explanations['score'].astype(float)\n",
    "explanations['support'] = explanations['support'].astype(float)\n",
    "explanations['2nd-inf'] = explanations['2nd-inf'].astype(float)/(-spd_0)\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "explanations.sort_values(by=['score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T10:33:26.920812Z",
     "start_time": "2021-06-28T10:33:26.293870Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lim = 0.1\n",
    "plt.figure(figsize=(5,5))\n",
    "xs = (np.arange(31)-15)/15*lim*0.8\n",
    "ys = xs\n",
    "plt.xlim(-lim, lim)\n",
    "plt.ylim(-lim, lim)\n",
    "plt.plot(xs, ys, 'grey')\n",
    "\n",
    "# RGBA\n",
    "color_first = np.zeros((len(gt_influences), 4))\n",
    "color_first[:, 2] = 1.0\n",
    "color_first[:, 3] = 1-np.array(fractionRows)/100\n",
    "color_second = np.zeros((len(gt_influences), 4))\n",
    "color_second[:, 0] = 1.0\n",
    "color_second[:, 3] = 1-np.array(fractionRows)/100\n",
    "\n",
    "# color_first = np.zeros((len(gt_influences), 4))\n",
    "# color_first[:, 0] = 1.0\n",
    "# color_first[:, 3] = 0.2\n",
    "# color_second = np.zeros((len(gt_influences), 4))\n",
    "# color_second[:, 0] = 1.0\n",
    "# color_second[:, 3] = 1.0\n",
    "\n",
    "plt.scatter(gt_influences, first_order_influences, s=12, color=color_first, label='first-order')\n",
    "plt.scatter(gt_influences, second_order_influences, s=12, color=color_second, label='second-order')\n",
    "plt.ylabel('estimated influence\\n', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('\\nground truth influence', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=12, prop={'weight':'bold'})\n",
    "plt.grid()\n",
    "plt.savefig('infs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T10:25:59.339757Z",
     "start_time": "2021-06-28T10:25:59.323465Z"
    }
   },
   "outputs": [],
   "source": [
    "time_gt_ave = []\n",
    "time_second_ave = []\n",
    "time_first_ave = []\n",
    "\n",
    "l = len(time_gt)//rep\n",
    "for i in range(l):\n",
    "    time_gt_ave.append(np.average([time_gt[i+j*l] for j in range(rep)]))\n",
    "    time_second_ave.append(np.average([time_second[i+j*l] for j in range(rep)]))\n",
    "    time_first_ave.append(np.average([time_first[i+j*l] for j in range(rep)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T10:33:35.439623Z",
     "start_time": "2021-06-28T10:33:35.009156Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "# plt.subplot(131)\n",
    "sorted_idx = explanations.sort_values(by=['fractionRows'], ascending=True).index\n",
    "xs = []\n",
    "ys = []\n",
    "for idx in sorted_idx:\n",
    "    xs.append(fractionRows[idx])\n",
    "    ys.append(time_first_ave[idx])\n",
    "plt.plot(xs, ys, '-', c='blue', label='first-order')\n",
    "\n",
    "# plt.subplot(132)\n",
    "xs = []\n",
    "ys = []\n",
    "for idx in sorted_idx:\n",
    "    xs.append(fractionRows[idx])\n",
    "    ys.append(time_second_ave[idx])\n",
    "plt.plot(xs, ys, '-', c='red', label='second-order')\n",
    "\n",
    "# plt.subplot(133)\n",
    "xs = []\n",
    "ys = []\n",
    "for idx in sorted_idx:\n",
    "    xs.append(fractionRows[idx])\n",
    "    ys.append(time_gt_ave[idx])\n",
    "plt.plot(xs, ys, '-', c='green', label='ground truth')\n",
    "plt.legend(fontsize=12, prop={'weight':'bold'})\n",
    "plt.grid()\n",
    "plt.xlabel('\\nfraction of data removed (%)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('time cost / sec\\n', fontsize=12, fontweight='bold')\n",
    "# plt.show()\n",
    "plt.savefig('time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T10:22:53.161226Z",
     "start_time": "2021-06-28T10:22:53.145026Z"
    }
   },
   "outputs": [],
   "source": [
    "# bucket_num = 10\n",
    "fractionRows = np.array(fractionRows)\n",
    "gt_influences = np.array(gt_influences)\n",
    "first_order_influences = np.array(first_order_influences)\n",
    "second_order_influences = np.array(second_order_influences)\n",
    "corr_first_gt_ls = []\n",
    "corr_second_gt_ls = []\n",
    "for bucket_id in range(10):\n",
    "    is_in_bucket = np.logical_and(fractionRows>=bucket_id*10, fractionRows<(bucket_id+1)*10)\n",
    "    corr_first_gt = np.corrcoef([gt_influences[is_in_bucket], first_order_influences[is_in_bucket]])[0][1]\n",
    "    if np.isnan(corr_first_gt):\n",
    "        corr_first_gt = 0\n",
    "    corr_second_gt = np.corrcoef([gt_influences[is_in_bucket], second_order_influences[is_in_bucket]])[0][1]\n",
    "    if np.isnan(corr_second_gt):\n",
    "        corr_second_gt = 0\n",
    "    corr_first_gt_ls.append(corr_first_gt)\n",
    "    corr_second_gt_ls.append(corr_second_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T10:22:53.899330Z",
     "start_time": "2021-06-28T10:22:53.513192Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "bar_width=0.3\n",
    "tick_label=[f'{i*10}%-{(i+1)*10}%' for i in range(10)]\n",
    "plt.bar(np.arange(10), corr_first_gt_ls, bar_width, color='blue', label='first')\n",
    "plt.bar(np.arange(10)+bar_width, corr_second_gt_ls, bar_width, color='orange', label='second')\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(np.arange(10)+bar_width/2, tick_label, rotation=20, fontsize=12)\n",
    "plt.xlabel('Row Fraction', fontsize=20)\n",
    "plt.ylabel('Pearson Correlation', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T10:23:06.072360Z",
     "start_time": "2021-06-28T10:23:06.062658Z"
    }
   },
   "outputs": [],
   "source": [
    "# overall correlation\n",
    "np.corrcoef([gt_influences, first_order_influences])[0][1], np.corrcoef([gt_influences, second_order_influences])[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T17:47:30.518658Z",
     "start_time": "2021-05-07T17:47:30.511518Z"
    }
   },
   "source": [
    "less than 2 coherent subsets correspond to the bracket 50%-60%, therefore cannot compute corrsponding pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T10:23:10.705692Z",
     "start_time": "2021-06-28T10:23:09.146689Z"
    }
   },
   "outputs": [],
   "source": [
    "# lim = 0.01\n",
    "plt.figure(figsize=(15, 6))\n",
    "for bucket_id in range(10):\n",
    "    plt.subplot(2, 5, bucket_id+1)\n",
    "    is_in_bucket = np.logical_and(fractionRows>=bucket_id*10, fractionRows<(bucket_id+1)*10)\n",
    "    xs = (np.arange(30)-15)/15*lim\n",
    "    ys = xs\n",
    "    plt.xlim(-lim, lim)\n",
    "    plt.ylim(-lim, lim)\n",
    "    plt.plot(xs, ys, 'g')\n",
    "    plt.scatter(gt_influences[is_in_bucket], first_order_influences[is_in_bucket], s=10, c='blue', label='first')\n",
    "    plt.scatter(gt_influences[is_in_bucket], second_order_influences[is_in_bucket], s=10, c='orange', label='second')\n",
    "    plt.legend()\n",
    "    plt.title(f'Fraction: {bucket_id*10}%-{(bucket_id+1)*10}% {np.sum(is_in_bucket)}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For buckets which include only small num of coherent subsets (many of them corresponds to high row fraction), the pearson correlation can be less meaningful. However, from the scatter plots above, we can see that the estimation of influence function tends to be more accurate when the row fractions are relatively low, meanwhile, low row fraction leads to low overall influence (change of metrics caused by removing the subset) of coherent subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
